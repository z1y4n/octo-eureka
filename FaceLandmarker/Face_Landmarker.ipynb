{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_cQX8dWu4Dv"
   },
   "source": [
    "# Face Landmarks Detection with MediaPipe Tasks\n",
    "\n",
    "This notebook shows you how to use MediaPipe Tasks Python API to detect face landmarks from images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6PN9FvIx614"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "OMjuVQiDYJKF"
   },
   "outputs": [],
   "source": [
    "# !wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYKAJ5nDU8-I"
   },
   "source": [
    "## Visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "s3E6NFV-00Qt"
   },
   "outputs": [],
   "source": [
    "#@markdown We implemented some functions to visualize the face landmark detection results. <br/> Run the following cell to activate the functions.\n",
    "import mediapipe as mp # type: ignore\n",
    "from mediapipe import solutions # type: ignore\n",
    "from mediapipe.framework.formats import landmark_pb2 # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import time # type: ignore\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  face_landmarks_list = detection_result.face_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected faces to visualize.\n",
    "  for idx in range(len(face_landmarks_list)):\n",
    "    face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "    # Draw the face landmarks.\n",
    "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    face_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "    ])\n",
    "\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp.solutions.drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "  return annotated_image\n",
    "\n",
    "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
    "  # Extract the face blendshapes category names and scores.\n",
    "  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
    "  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
    "  # The blendshapes are ordered in decreasing score value.\n",
    "  face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(12, 12))\n",
    "  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
    "  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
    "  ax.invert_yaxis()\n",
    "\n",
    "  # Label each bar with values\n",
    "  for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
    "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
    "\n",
    "  ax.set_xlabel('Score')\n",
    "  ax.set_title(\"Face Blendshapes\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "last_is_blink = False\n",
    "last_doze_time = 0\n",
    "last_wake_time = 0\n",
    "doze_counter = 0\n",
    "doze_timer = 0\n",
    "doze_start = 0\n",
    "is_doze = False\n",
    "def print_eyes_status(face_blendshapes):\n",
    "  global last_is_blink\n",
    "  global last_doze_time\n",
    "  global last_wake_time\n",
    "  global doze_counter\n",
    "  global doze_timer\n",
    "  global doze_start\n",
    "  global is_doze\n",
    "  face_blendshapes_scores = np.array([face_blendshapes_category.score for face_blendshapes_category in face_blendshapes])\n",
    "  upward_score = face_blendshapes_scores[17] + face_blendshapes_scores[18]\n",
    "  downward_score = face_blendshapes_scores[11] + face_blendshapes_scores[12]\n",
    "  leftward_score = face_blendshapes_scores[14] + face_blendshapes_scores[15]\n",
    "  rightward_score = face_blendshapes_scores[13] + face_blendshapes_scores[16]\n",
    "  dir_score = np.array([[\"upward\",upward_score], [\"downward\",downward_score], [\"leftward\",leftward_score], [\"rightward\",rightward_score]])\n",
    "  dir = dir_score[np.argmax(dir_score[:,1])][0]\n",
    "  #print(dir)\n",
    "  is_blink = face_blendshapes_scores[9]>0.6 and face_blendshapes_scores[10]>0.6\n",
    "  if is_blink:\n",
    "    if not last_is_blink:\n",
    "      last_doze_time = time.time()\n",
    "    if last_doze_time != 0 and time.time() > last_doze_time:\n",
    "      doze_counter += 1\n",
    "      time.sleep(0.01)\n",
    "    if doze_counter > 70:\n",
    "      if not is_doze:\n",
    "        doze_start = time.time()\n",
    "      is_doze = True\n",
    "      last_wake_time = 0\n",
    "\n",
    "  else:\n",
    "    if last_is_blink:\n",
    "      last_wake_time = time.time()\n",
    "    if last_wake_time != 0 and time.time() > last_wake_time:\n",
    "      doze_counter -= 1\n",
    "      time.sleep(0.01)\n",
    "    if doze_counter < 20:\n",
    "      doze_counter = 0\n",
    "      is_doze = False\n",
    "  doze_timer = time.time() - doze_start    \n",
    "  last_is_blink = is_blink\n",
    "  #print(is_blink)\n",
    "  if is_doze:\n",
    "    print(f\"doze counter {doze_counter}\",f\"dozing for {doze_timer:.2f}s\")\n",
    "  else:\n",
    "    print(f\"{'blinking' if is_blink else f'looking {dir}'}\")\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy4r2_ePylIa"
   },
   "source": [
    "## Running inference and visualizing the results\n",
    "\n",
    "Here are the steps to run face landmark detection using MediaPipe.\n",
    "\n",
    "Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/face_landmarker/python) to learn more about configuration options that this task supports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "_JVO3rvPD4RN"
   },
   "outputs": [],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an FaceLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                       output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "detector = vision.FaceLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking leftward\n",
      "looking leftward\n",
      "looking leftward\n",
      "looking leftward\n",
      "looking leftward\n",
      "looking leftward\n",
      "looking leftward\n",
      "looking leftward\n",
      "looking leftward\n",
      "looking leftward\n",
      "looking leftward\n",
      "looking leftward\n",
      "looking downward\n",
      "looking rightward\n",
      "looking rightward\n",
      "looking rightward\n",
      "looking rightward\n",
      "looking upward\n",
      "looking upward\n",
      "looking upward\n",
      "looking upward\n",
      "looking downward\n",
      "looking downward\n",
      "looking downward\n",
      "looking downward\n",
      "looking leftward\n",
      "looking leftward\n",
      "looking upward\n",
      "looking upward\n",
      "looking upward\n",
      "looking upward\n",
      "looking upward\n",
      "looking downward\n",
      "looking downward\n",
      "blinking\n",
      "blinking\n",
      "blinking\n",
      "blinking\n",
      "blinking\n",
      "blinking\n",
      "blinking\n",
      "looking downward\n",
      "looking downward\n",
      "blinking\n",
      "blinking\n",
      "blinking\n",
      "blinking\n",
      "looking downward\n",
      "looking upward\n",
      "looking upward\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import cv2 # type: ignore\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "# print(ret_cam)\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    if frame is None:\n",
    "        continue\n",
    "    cam_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)   \n",
    "    detection_result_cam = detector.detect(cam_image)\n",
    "    if len(detection_result_cam.face_blendshapes)==0 or len(detection_result_cam.face_landmarks)==0:\n",
    "        continue\n",
    "    annotated_image = draw_landmarks_on_image(cam_image.numpy_view(), detection_result_cam)\n",
    "    cv2.namedWindow('cam', cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.imshow(\"cam\", annotated_image)\n",
    "    print_eyes_status(detection_result_cam.face_blendshapes[0])\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'): \n",
    "        print(\"Exiting...\")          \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKelLdIfwL4V"
   },
   "source": [
    "We will also visualize the face blendshapes categories using a bar graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckKurV96cG01"
   },
   "source": [
    "And print the transformation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "xixKF10-rmse"
   },
   "outputs": [],
   "source": [
    "#(detection_result.facial_transformation_matrixes)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
